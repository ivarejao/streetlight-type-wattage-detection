{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# Data\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision import transforms,models\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "from torch import optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "# Plots\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time, os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULT_NAME = 'resnet_pre_trained_fine_tuning.csv'\n",
    "NET_NAME = \"resnet_pre_trained_fine_tuning_\"\n",
    "\n",
    "# hyperparameters\n",
    "args = {\n",
    "    'epoch_num': 100,     # Epochs\n",
    "    'lr': 0.01,           # Lr\n",
    "    'weight_decay': 1e-3, # L2\n",
    "    'batch_size': 32,     # batch size\n",
    "    'num_classes': 9\n",
    "}\n",
    "\n",
    "# hardware\n",
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda')\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"./\"\n",
    "base_dir = \"./\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LampDataset(Dataset):\n",
    "    def __init__(self, main_dir, transform, labels):\n",
    "        self.main_dir = main_dir\n",
    "        self.transform = transform\n",
    "        all_imgs = os.listdir(main_dir)\n",
    "        all_imgs = [i for i in all_imgs if i.endswith('.jpg')]\n",
    "        self.total_imgs = sorted(all_imgs)\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.total_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_loc = os.path.join(self.main_dir, self.total_imgs[idx])\n",
    "        image = Image.open(img_loc,'r')\n",
    "        tensor_image = self.transform(image)\n",
    "\n",
    "        return tensor_image, self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(base_dir+'dataset.csv',sep=\";\")\n",
    "\n",
    "df.set_index('id',inplace=True)\n",
    "\n",
    "df['y'] = df['tipo_lampada'].str.replace(\" \", \"\") + df['potencia'].astype(str)\n",
    "\n",
    "labels_mapping = dict(enumerate(df['y'].astype('category').cat.categories))\n",
    "\n",
    "labels = df['y'].astype('category').cat.codes\n",
    "\n",
    "labels = torch.tensor(labels.values).type(torch.LongTensor)\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOURIER = ['df01', 'df02', 'df03', 'df04','df05', 'df06', 'df07', 'df08', 'df09', 'df10']\n",
    "HU = ['i1', 'i2', 'i3', 'i4','i5', 'i6', 'i7']\n",
    "HARALICK = ['probmax', 'energia', 'entropia', 'contraste','homogeneidade', 'correlacao']\n",
    "\n",
    "all_features = FOURIER + HU + HARALICK\n",
    "\n",
    "X = df[all_features]\n",
    "\n",
    "X = X.apply(lambda x: x.str.replace(',', '.').astype(float), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                                transforms.Resize((144, 216),antialias=True),\n",
    "                                transforms.ToTensor()\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LampDataset(base_dir+'img/',transform,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_loader = DataLoader(\n",
    "  dataset,\n",
    "  batch_size=len(dataset),\n",
    "  shuffle=False,\n",
    "  num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "images, labels = next(iter(image_data_loader))\n",
    "\n",
    "def display_image(images):\n",
    "  images_np = images.numpy()\n",
    "  img_plt = images_np.transpose(0,2,3,1)\n",
    "  # display 5th image from dataset\n",
    "  plt.imshow(img_plt[4])\n",
    "\n",
    "display_image(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_adapted_vit():\n",
    "    model = models.vit_b_16(weights=models.ViT_B_16_Weights.DEFAULT).to(args['device'])\n",
    "    num_classes =  args['num_classes']  # Change this to your desired number of classes\n",
    "    model.heads.head = nn.Linear(in_features=model.heads.head.in_features, out_features=num_classes).to(args['device'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, net, epoch):\n",
    "\n",
    "  # Training mode\n",
    "  net.train()\n",
    "\n",
    "  start = time.time()\n",
    "\n",
    "  epoch_loss  = []\n",
    "  pred_list, rotulo_list = [], []\n",
    "  for batch in train_loader:\n",
    "\n",
    "    dado, rotulo = batch\n",
    "\n",
    "    # Cast data to GPU\n",
    "    dado = dado.to(args['device'])\n",
    "    rotulo = rotulo.to(args['device'])\n",
    "   \n",
    "    # Forward\n",
    "    ypred = net(dado)\n",
    "    loss = criterion(ypred, rotulo)\n",
    "    epoch_loss.append(loss.cpu().data)\n",
    "\n",
    "    _, pred = torch.max(ypred, axis=1)\n",
    "    pred_list.append(pred.cpu().numpy())\n",
    "    rotulo_list.append(rotulo.cpu().numpy())\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "  #before_lr = optimizer.param_groups[0][\"lr\"]\n",
    "  scheduler.step()\n",
    "  #after_lr = optimizer.param_groups[0][\"lr\"]\n",
    "  #print(\"Epoch %d: SGD lr %.4f -> %.4f\" % (epoch, before_lr, after_lr))\n",
    "  epoch_loss = np.asarray(epoch_loss)\n",
    "  pred_list  = np.concatenate(pred_list).ravel()\n",
    "  rotulo_list  = np.concatenate(rotulo_list).ravel()\n",
    "\n",
    "\n",
    "  acc = accuracy_score(pred_list, rotulo_list)\n",
    "\n",
    "  end = time.time()\n",
    "  #print('#################### Train ####################')\n",
    "  #print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Time: %.2f' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, end-start))\n",
    "\n",
    "  return epoch_loss.mean(),acc, end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(test_loader, net, epoch):\n",
    "\n",
    "  # Evaluation mode\n",
    "  net.eval()\n",
    "\n",
    "  start = time.time()\n",
    "\n",
    "  epoch_loss  = []\n",
    "  pred_list, rotulo_list = [], []\n",
    "  with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "\n",
    "      dado, rotulo = batch\n",
    "\n",
    "      # Cast do dado na GPU\n",
    "      dado = dado.to(args['device'])\n",
    "      rotulo = rotulo.to(args['device'])\n",
    "\n",
    "      # Forward\n",
    "      ypred = net(dado)\n",
    "      loss = criterion(ypred, rotulo)\n",
    "      epoch_loss.append(loss.cpu().data)\n",
    "\n",
    "      _, pred = torch.max(ypred, axis=1)\n",
    "      pred_list.append(pred.cpu().numpy())\n",
    "      rotulo_list.append(rotulo.cpu().numpy())\n",
    "\n",
    "  epoch_loss = np.asarray(epoch_loss)\n",
    "\n",
    "  pred_list  = np.concatenate(pred_list).ravel()\n",
    "  rotulo_list  = np.concatenate(rotulo_list).ravel()\n",
    "\n",
    "  acc = accuracy_score(pred_list, rotulo_list)\n",
    "\n",
    "  end = time.time()\n",
    "  #print('********** Validate **********')\n",
    "  #print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Time: %.2f\\n' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, end-start))\n",
    "  #print('Epoch %d, Loss: %.4f +/- %.4f, Acc: %.2f, Time: %.2f\\n' % (epoch, epoch_loss.mean(), epoch_loss.std(), acc*100, end-start))\n",
    "\n",
    "  return epoch_loss.mean(),acc, end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "REPEATED_CV = RepeatedStratifiedKFold(n_splits=10, n_repeats=3,random_state=18062001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def validation_index(labels,train_index):\n",
    "  train_index, valid_index, _, _ = train_test_split(train_index, labels,stratify=labels, test_size=0.1, random_state=18062001)\n",
    "\n",
    "\n",
    "  return train_index, valid_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold, (train_index, test_index) in enumerate(REPEATED_CV.split(X,labels)):\n",
    "\n",
    "    # Print\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "    print('| Epoch | Train Loss | Train Acc | Validation Loss | Validation Acc | Time |')\n",
    "\n",
    "    train_index2, valid_index = validation_index(labels[train_index],train_index)\n",
    "\n",
    "    # Sample elements randomly from a given list of ids, no replacement.\n",
    "    train_subsampler = torch.utils.data.SubsetRandomSampler(train_index2)\n",
    "    valid_subsampler = torch.utils.data.SubsetRandomSampler(valid_index)\n",
    "\n",
    "    # Define data loaders for training and testing data in this fold\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=args['batch_size'],\n",
    "                      sampler=train_subsampler)\n",
    "\n",
    "    valid_loader = torch.utils.data.DataLoader(\n",
    "                      dataset,\n",
    "                      batch_size=args['batch_size'],\n",
    "                        sampler=valid_subsampler)\n",
    "\n",
    "    net = new_adapted_vit()\n",
    "    for param in net.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    for param in net.heads.parameters():\n",
    "        param.requires_grad = True\n",
    "    #net.apply(reset_weights)\n",
    "    criterion = nn.CrossEntropyLoss().to(args['device'])\n",
    "    optimizer = optim.Adam(net.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "    scheduler = lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "    \n",
    "    train_losses, test_losses = [], []\n",
    "\n",
    "    best_acc = 0\n",
    "    for epoch in range(args['epoch_num']):\n",
    "\n",
    "      # Train\n",
    "      train_loss, train_acc, train_time = train(train_loader, net, epoch)\n",
    "      train_losses.append(train_loss)\n",
    "\n",
    "      # Validate\n",
    "      test_loss, test_acc, test_time = validate(valid_loader, net, epoch)\n",
    "      test_losses.append(test_loss)\n",
    "\n",
    "      print(f'|  {epoch:03.0f}  |   {train_loss:.5f}  |    {train_acc*100:02.0f}%    |     {test_loss:.5f}     |       {test_acc*100:02.0f}%      | {train_time + test_time:.2f} |')\n",
    "\n",
    "      if test_acc >= best_acc:\n",
    "        torch.save(net.state_dict(), root_dir + 'tcc/models/' + f'best-model-parameters-fold{fold}.pt')\n",
    "\n",
    "    #plot fold losses\n",
    "    plt.close()\n",
    "    plt.plot(train_losses,label= 'train loss')\n",
    "    plt.plot(test_losses, label= 'test loss')\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.savefig(root_dir + 'tcc/loss/' + f\"fold{fold}_loss_plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "if TEST_TRAIN:\n",
    "    scores = []\n",
    "    for fold, (train_index, test_index) in enumerate(REPEATED_CV.split(X,labels)):\n",
    "    \n",
    "      PATH = root_dir + 'tcc/models/' + f'best-model-parameters-fold{fold}.pt'\n",
    "    \n",
    "      #Load Trained Model\n",
    "      test_net = net\n",
    "      test_net.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "      print(f'FOLD {fold}')\n",
    "      print('--------------------------------')\n",
    "      test_subsampler = torch.utils.data.SubsetRandomSampler(test_index)\n",
    "    \n",
    "      test_loader = torch.utils.data.DataLoader(\n",
    "                        dataset,\n",
    "                        batch_size=args['batch_size'],\n",
    "                          sampler=test_subsampler)\n",
    "    \n",
    "      # Test\n",
    "      test_loss, test_acc, _ = validate(test_loader, test_net, 0)\n",
    "      print(f\"{test_acc}\")\n",
    "      scores.append(test_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats \n",
    "methods_results = {}\n",
    "\n",
    "scores_np = np.array(scores)\n",
    "\n",
    "methods_results[NET_NAME] = scores_np.copy()\n",
    "\n",
    "inf, sup = stats.norm.interval(0.95, loc=scores_np.mean(),\n",
    "                                scale=scores_np.std()/np.sqrt(len(scores_np)))\n",
    "\n",
    "print(\"Mean: {} +/- {} \\n Inf: {} Sup : {}\".format(scores_np.mean(),scores_np.std(),inf,sup))\n",
    "\n",
    "result_comp = {}\n",
    "result_comp[\"ViT\"] = scores_np\n",
    "df_resultsA = pd.DataFrame.from_dict(result_comp)\n",
    "df_resultsA.to_csv(\"vit_results_final.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrair caracteristicas com ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extrai_caracteristicas_vit(net, loader):\n",
    "\n",
    "  # Evaluation mode\n",
    "  net.eval()\n",
    "  vit = net\n",
    "  feat_list, rotulo_list = [], []\n",
    "  \n",
    "  with torch.no_grad():\n",
    "    for k, batch in enumerate(loader):\n",
    "      print('\\r--{0}/{1}--'.format(k, len(loader)), end='', flush=True)\n",
    "\n",
    "      dado, rotulo = batch\n",
    "\n",
    "      # Cast do dado na GPU\n",
    "      dado = dado.to(args['device'])\n",
    "      rotulo = rotulo.to(args['device'])\n",
    "\n",
    "      # Extração\n",
    "      feats = vit._process_input(dado)\n",
    "\n",
    "    # Expand the class token to the full batch\n",
    "      batch_class_token = vit.class_token.expand(dado.shape[0], -1, -1)\n",
    "      feats = torch.cat([batch_class_token, feats], dim=1)\n",
    "\n",
    "      feats = vit.encoder(feats)\n",
    "\n",
    "    # We're only interested in the representation of the classifier token that we appended at position 0\n",
    "      feats = feats[:, 0]\n",
    "      feat_list.append(feats.detach().cpu().numpy())\n",
    "      rotulo_list.append(rotulo.detach().cpu().numpy())\n",
    "\n",
    "  feat_list = np.asarray(feat_list)\n",
    "  feat_list = np.reshape(feat_list, (feat_list.shape[0]*feat_list.shape[1], feat_list.shape[2]))\n",
    "\n",
    "  rotulo_list = np.asarray(rotulo_list).ravel()\n",
    "\n",
    "  return feat_list, rotulo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_net = new_adapted_vit_to_extract()\n",
    "\n",
    "with torch.no_grad():\n",
    "    vit_X, vit_Y = extrai_caracteristicas_vit(extract_net, image_data_loader)\n",
    "\n",
    "df_feats = pd.DataFrame(vit_X)\n",
    "df_feats['class'] = vit_Y\n",
    "df_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feats.to_csv(\"vit_features.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cross_validation(model,model_name,model_parameters,X, y,standardize = True):\n",
    "\n",
    "    labels = np.unique(y)\n",
    "    labels = np.sort(labels)\n",
    "    \n",
    "    REPEATED_CV = RepeatedStratifiedKFold(n_splits=10, n_repeats=3,random_state=18062001)\n",
    "    \n",
    "    scores = {\n",
    "        'accuracy':[],\n",
    "        'confusion_matrix': []\n",
    "    }\n",
    "\n",
    "    #labels = np.unique(y)\n",
    "    #labels = np.sort(labels)\n",
    "\n",
    "    #extern loop\n",
    "    for i, (train_index, test_index) in enumerate(REPEATED_CV.split(X, y)):\n",
    "        \n",
    "        # Split Data in train and test\n",
    "        x_train, x_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        #check with standardization\n",
    "        if standardize == True:\n",
    "            pipe_clf = Pipeline([('scaler', StandardScaler()), (model_name, model)])\n",
    "        else:\n",
    "            pipe_clf = Pipeline([(model_name, model)])\n",
    "\n",
    "        # intern loop \n",
    "        clf = GridSearchCV(pipe_clf, model_parameters,cv=4)\n",
    "  \n",
    "        clf.fit(x_train, y_train)\n",
    "  \n",
    "        #test\n",
    "        y_pred = clf.predict(x_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "        scores['accuracy'].append(acc)\n",
    "        #scores['confusion_matrix'].append(confusion_matrix(y_test, y_pred,labels = labels))\n",
    "\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_features(features_name):\n",
    "\n",
    "    parameters_svm = {'svm__C': [0.1,1, 10, 100], \n",
    "                'svm__gamma': [1,0.1,0.01,0.001],\n",
    "                'svm__kernel': ['rbf', 'poly', 'sigmoid']}\n",
    "\n",
    "    clf = SVC()\n",
    "\n",
    "    df_data = pd.read_csv(features_name)\n",
    "\n",
    "    X = df_data.loc[:, df_data.columns != 'class']\n",
    "    y = df_data['class']\n",
    "    \n",
    "    X = X.to_numpy()\n",
    "    y = y.to_numpy()\n",
    "\n",
    "    labels = list(labels_mapping.values())\n",
    "\n",
    "    s = nested_cross_validation(clf,'svm',parameters_svm,X,y)\n",
    "    \n",
    "    scores_np = np.array(s['accuracy'])\n",
    "    inf, sup = stats.norm.interval(0.95, loc=scores_np.mean(),\n",
    "                                  scale=scores_np.std()/np.sqrt(len(scores_np)))\n",
    "    \n",
    "    print(\"Mean: {} +/- {} \\n Inf: {} Sup : {}\".format(scores_np.mean(),scores_np.std(),inf,sup))\n",
    "\n",
    "    return scores_np, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features('vit_features.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
